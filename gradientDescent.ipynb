{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import sklearn as sk\n",
    "import matplotlib as plt\n",
    "\n",
    "#Begin rewrite here.\n",
    "\n",
    "print('Plotting Data ...\\n')\n",
    "data = np.loadtxt('ex1data1.txt')\n",
    "X = data(:, 0)\n",
    "y = data(:, 1)\n",
    "m = len(y); # number of training examples\n",
    "\n",
    "# Plot Data\n",
    "plot(X, y, 'rx', 'MarkerSize', 10)       # Plot the data\n",
    "plt.ylabel('Profit in $10,000s')             # Set the y-axis label\n",
    "plt.xlabel('Population of City in 10,000s')  # Set the x-axis label\n",
    "\n",
    "\n",
    "X = [ones(m, 1), data(:, 1)] # Add a column of ones to x\n",
    "theta = zeros(2, 1)          # initialize fitting parameters\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "\n",
    "print('\\nTesting the cost function ...\\n')\n",
    "# compute and display initial cost\n",
    "\n",
    "def computeCost(X, y, theta):\n",
    "    J = 0\n",
    "    m = len(y)\n",
    "    predictions = X * theta\n",
    "    sqrErrors = (predictions - y) ** 2\n",
    "\n",
    "    J = 1 / (2 * m) * sum(sqrErrors)\n",
    "\n",
    "\n",
    "\n",
    "J = computeCost(X, y, theta)\n",
    "print('With theta = [0 ; 0] \\n Cost computed = %0.3f \\n', J)\n",
    "\n",
    "# further testing of the cost function\n",
    "J = computeCost(X, y, [-1 ; 2])\n",
    "print('\\n With theta = [-1 ; 2] \\n Cost computed = %0.3f \\n', J)\n",
    "print('Expected cost value (approx) 54.24\\n')\n",
    "\n",
    "\n",
    "print('\\n Running Gradient Descent ...\\n')\n",
    "\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    # Initialize some useful values\n",
    "    \n",
    "    m = len(y); # number of training examples\n",
    "    J_history = np.zeros(num_iters, 1)\n",
    "    for iter in range(num_iters):\n",
    "        x = X(:, 2)\n",
    "        hypothesis = theta(1) + (x * theta(2))\n",
    "        stdError = hypothesis - y\n",
    "    \n",
    "        #store theta1 as a temp value\n",
    "        temp_zero = theta(1) - alpha * (1/m) * sum(stdError)\n",
    "        temp_one = theta(2) - alpha * (1/m) * sum(stdError * x)\n",
    "    \n",
    "        #the hard part of the simultaneous update\n",
    "        theta = [temp_zero; temp_one]\n",
    "    \n",
    "        J_history(iter) = computeCost(X, y, theta)\n",
    "\n",
    "        # Save the cost J in every iteration    \n",
    "        J_history.append(computeCost(X, y, theta))\n",
    "        \n",
    "    return theta, J_history\n",
    "\n",
    "\n",
    "# run gradient descent\n",
    "theta = gradientDescent(X, y, theta, alpha, iterations);\n",
    "\n",
    "# print theta to screen\n",
    "print('Theta found by gradient descent: \\n')\n",
    "print('%f \\n', theta)\n",
    "\n",
    "# Plot the linear fit\n",
    "hold on; # keep previous plot visible\n",
    "plot(X(:,2), X*theta, '-')\n",
    "plt.legend('Training data', 'Linear regression')\n",
    "hold off # don't overlay any more plots on this figure\n",
    "\n",
    "# Predict values for population sizes of 35,000 and 70,000\n",
    "predict1 = [1, 3.5] *theta;\n",
    "print('For population = 35,000, we predict a profit of #f\\n',...\n",
    "    predict1*10000);\n",
    "predict2 = [1, 7] * theta;\n",
    "print('For population = 70,000, we predict a profit of #f\\n',...\n",
    "    predict2*10000);\n",
    "\n",
    "print('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "# ============= Part 4: Visualizing J(theta_0, theta_1) =============\n",
    "print('Visualizing J(theta_0, theta_1) ...\\n')\n",
    "\n",
    "# Grid over which we will calculate J\n",
    "theta0_vals = linespace(-10, 10, 100);\n",
    "theta1_vals = linespace(-1, 4, 100);\n",
    "\n",
    "# initialize J_vals to a matrix of 0's\n",
    "J_vals = zeros(length(theta0_vals), length(theta1_vals));\n",
    "\n",
    "# Fill out J_vals\n",
    "for i in range(1, len(theta0_vals)):\n",
    "    for j = 1:length(theta1_vals):\n",
    "        t = [theta0_vals(i); theta1_vals(j)];\n",
    "        J_vals(i,j) = computeCost(X, y, t); \n",
    "        \n",
    "# Because of the way meshgrids work in the surf command, we need to\n",
    "# transpose J_vals before calling surf, or else the axes will be flipped\n",
    "J_vals = J_vals.T\n",
    "# Surface plot\n",
    "figure\n",
    "surf(theta0_vals, theta1_vals, J_vals)\n",
    "plt.xlabel('\\theta_0'); plt.ylabel('\\theta_1')\n",
    "\n",
    "# Contour plot\n",
    "figure\n",
    "# Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\n",
    "contour(theta0_vals, theta1_vals, J_vals, logspace(-2, 3, 20))\n",
    "plt.xlabel('\\theta_0'); ylabel('\\theta_1')\n",
    "\n",
    "plot(theta(1), theta(2), 'rx', 'MarkerSize', 10, 'LineWidth', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
